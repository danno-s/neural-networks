% Template:     Informe/Reporte LaTeX
% Documento:    Archivo principal
% Versión:      4.7.4 (04/04/2018)
% Codificación: UTF-8
%
% Autor: Pablo Pizarro R.
%        Facultad de Ciencias Físicas y Matemáticas
%        Universidad de Chile
%        pablo.pizarro@ing.uchile.cl, ppizarror.com
%
% Manual template: [http://latex.ppizarror.com/Template-Informe/]
% Licencia MIT:    [https://opensource.org/licenses/MIT/]

% CREACIÓN DEL DOCUMENTO
\documentclass[letterpaper,11pt]{article} % Articulo tamaño carta, 11pt
\usepackage[utf8]{inputenc} % Codificación UTF-8

% INFORMACIÓN DEL DOCUMENTO
\def\titulodelinforme {Redes Neuronales}
\def\temaatratar {Leyendo números}

\def\autordeldocumento {Daniel Soto}
\def\nombredelcurso {Redes Neuronales y Programación Genética}
\def\codigodelcurso {CC5114}

\def\nombreuniversidad {Universidad de Chile}
\def\nombrefacultad {Facultad de Ciencias Físicas y Matemáticas}
\def\departamentouniversidad {Departamento de Ciencias de la Computación}
\def\imagendepartamento {departamentos/dcc}
\def\imagendepartamentoescala {0.2}
\def\localizacionuniversidad {Santiago, Chile}

% INTEGRANTES, PROFESORES Y FECHAS
\def\tablaintegrantes {
\begin{tabular}{ll}
	Alumno:
		& \begin{tabular}[t]{@{}l@{}}
			Daniel Soto
		\end{tabular} \\
  Profesor:
    & \begin{tabular}[t]{@{}l@{}}
      Alexander Bergel \\
    \end{tabular} \\
  Auxiliar:
    & \begin{tabular}[t]{@{}l@{}}
      Juan-Pablo Silva \\
    \end{tabular} \\
  Ayudantes:
    & \begin{tabular}[t]{@{}l@{}}
      Alonso Reyes \\
      Gabriel Chandia \\
    \end{tabular} \\
	\multicolumn{2}{l}{Fecha de entrega: \today} \\
	\multicolumn{2}{l}{\localizacionuniversidad}
\end{tabular}
}

% CONFIGURACIONES
\input{lib/config}

% IMPORTACIÓN DE LIBRERÍAS
\input{lib/imports}

% IMPORTACIÓN DE FUNCIONES
\input{lib/function/core}
\input{lib/function/elements}
\input{lib/function/equation}
\input{lib/function/image}
\input{lib/function/title}

% IMPORTACIÓN DE ENTORNOS
\input{lib/environments}

% IMPORTACIÓN DE ESTILOS
\input{lib/styles}

% CONFIGURACIÓN INICIAL DEL DOCUMENTO
\input{lib/initconf}

% INICIO DE LAS PÁGINAS
\begin{document}

% PORTADA
\input{lib/portrait}

% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\input{lib/pageconf}

% CONFIGURACIONES FINALES
\input{lib/finalconf}

% ======================= INICIO DEL DOCUMENTO =======================

\section*{Repositorio}
  Todo el código y datos mencionados en este informe se encuentran en el \href{https://github.com/danno-s/neural-networks}{siguiente repositorio}.

\section*{Dataset y objetivos}
  Se utilizó un dataset de \href{https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits}{dígitos escritos a mano}, con el objetivo de lograr reconocer qué valor tiene cada imágen. Estas imágenes son de 4 bits por pixel, y de $8 \times 8$ pixeles. Todos los datos se encuentran marcados con el dígito que representan.

\section*{Ejecución de la red}
  La red completa fue programada en Python 3.6, sin utilizar ninguna librería externa. Para ejecutarla con este dataset, basta con ejecutar en la terminal \texttt{\$ python3 src/main.py}.

\section*{Experimentos y análisis}
  Se comenzó probando redes de capa intermedia, hasta 5 capas intermedias, todas de 16 neuronas y con \texttt{learning\_rate=0.5}. Los aciertos mostrados son el promedio de 5 ejecuciones.

  \begin{figure}
    \centering
    \begin{tabular}{|c|c|}
      \hline
      Número de capas intermedias & Aciertos \\
      \hline
      $1$ & $43.68\%$ \\
      \hline
      $2$ & $44.48\%$ \\
      \hline
      $3$ & $47.27\%$ \\
      \hline
      $4$ & $36.38\%$ \\
      \hline
      $5$ & $24.80\%$ \\
      \hline
    \end{tabular}
    \caption{Efecto de variar el número de capas en la red.}
  \end{figure}


  Claramente el número óptimo de capas intermedias parece ser 3. Luego se continuó experimentando con el número de neuronas dentro de estas capas. Se probaron los siguientes números de neurona para las 3 capas:

  \begin{itemize}
    \item $(16, 16, 16)$
    \item $(64, 16, 16)$
    \item $(64, 64, 16)$
    \item $(64, 64, 64)$
    \item $(64, 128, 64)$
  \end{itemize}

  \begin{figure}
    \centering
    \begin{tabular}{|c|c|}
      \hline
      Número de neuronas & Aciertos \\
      \hline
      $(16, 16, 16)$ & $47.27\%$ \\
      \hline
      $(64, 16, 16)$ & $54.10\%$ \\
      \hline
      $(64, 64, 16)$ & $66.12\%$ \\
      \hline
      $(64, 64, 64)$ & $59.82\%$ \\
      \hline
      $(64, 128, 64)$ & $57.39\%$ \\
      \hline
    \end{tabular}
    \caption{Efecto del número de neuronas en las capas intermedias}
  \end{figure}

  De esto, pareciera ser que es bueno ir reduciendo de a poco la cantidad de neuronas en las redes, hasta llegar al número de outputs necesario. Esto nos lleva a probar con una configuración específica. Al realizar el mismo experimento con 4 capas ocultas, $(128, 64, 64, 16)$, se obtuvo un porcentaje de aciertos promedio de $70.69\%$.

% FIN DEL DOCUMENTO
\end{document}
